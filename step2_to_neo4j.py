"""
STEP 2: Enhance JSONs and insert into Neo4j
Run: python step2_to_neo4j.py
"""

import json, re, os, sys, pathlib, unicodedata
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

NEO4J_URI      = os.getenv("NEO4J_URI",      "bolt://localhost:7687")
NEO4J_USER     = os.getenv("NEO4J_USER",     "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "neo4j123")

JSON_DIR   = pathlib.Path("jsons")
OUTPUT_DIR = pathlib.Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

LEGAL_KEYWORDS_AR = [
    "Ø¹Ù‚ÙˆØ¨Ø©","ØºØ±Ø§Ù…Ø©","Ø­Ø¨Ø³","Ø³Ø¬Ù†","Ù…ØµØ§Ø¯Ø±Ø©","ØªØ¹ÙˆÙŠØ¶","ØªØ±Ø®ÙŠØµ","ØªØµØ±ÙŠØ­",
    "ØªØ¹Ø±ÙŠÙ","Ø­Ø¸Ø±","ÙŠØ­Ø¸Ø±","ÙŠØ¬ÙˆØ²","ÙŠÙ„ØªØ²Ù…","ÙŠÙØ¹Ø§Ù‚Ø¨","Ù…Ø§Ø¯Ø©","Ø¥Ù„ØºØ§Ø¡",
    "ØªØ¹Ø¯ÙŠÙ„","Ø¥Ø¶Ø§ÙØ©","Ø­Ù‚ÙˆÙ‚","ÙˆØ§Ø¬Ø¨Ø§Øª","Ø§Ù„ØªØ²Ø§Ù…","Ø¬Ø±ÙŠÙ…Ø©","Ù…Ø­ÙƒÙ…Ø©","Ù‚Ø§Ø¶ÙŠ",
    "Ù†ÙŠØ§Ø¨Ø©","Ø§Ø³ØªØ¦Ù†Ø§Ù","Ø­ÙƒÙ…","Ù‚Ø±Ø§Ø±","ÙˆØ²ÙŠØ±","Ø±Ø¦ÙŠØ³","ØªØ³Ø¬ÙŠÙ„","Ø´Ù‡Ø§Ø¯Ø©",
    "Ø±Ù‡Ù†","Ø¶Ù…Ø§Ù†","Ø¥Ø´Ù‡Ø§Ø±","Ø£ÙˆÙ„ÙˆÙŠØ©","Ø§Ø³ØªÙŠØ±Ø§Ø¯","ØªØµØ¯ÙŠØ±","Ø¹Ø¨ÙˆØ±","Ø´Ø­Ù†Ø©",
    "ØµÙ†Ø§Ø¹ÙŠ","ØªØ¨Ø±ÙŠØ¯","Ù…Ø±Ø®Øµ Ù„Ù‡","Ù…Ø´ØªØ±Ùƒ","Ù…Ø³ØªÙ‡Ù„Ùƒ","ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©","Ù…Ø§Ø¦ÙŠØ©",
    "Ù…Ù†Ø´Ø¢Øª","Ø­Ù…Ø§ÙŠØ©",
]

def normalize(text):
    text = unicodedata.normalize("NFKC", text)
    return re.sub(r"\s+", " ", text).strip()

def extract_article_number(title):
    m = re.search(r"(\d+)", title)
    if m:
        return m.group(1)
    m2 = re.search(r"Ø§Ù„Ù…Ø§Ø¯Ø©\s+[-â€“]?\s*(.+)$", title)
    if m2:
        return m2.group(1).strip()
    return title

def extract_keywords(text):
    return list(dict.fromkeys(kw for kw in LEGAL_KEYWORDS_AR if kw in text))

def infer_law_type(text):
    if "ØªØ¹Ø¯ÙŠÙ„" in text: return "amendment"
    if "Ø¥ØµØ¯Ø§Ø±" in text: return "promulgation"
    if "Ø¥Ù„ØºØ§Ø¡" in text: return "repeal"
    return "original"

def build_law_id(file_stem, meta):
    num = meta.get("Ø±Ù‚Ù…", "")
    year_raw = meta.get("Ø§Ù„ØªØ§Ø±ÛŒØ®", meta.get("Ø§Ù„ØªØ§Ø±ÙŠØ®", ""))
    year_m = re.search(r"(\d{4})", year_raw)
    year = year_m.group(1) if year_m else "unknown"
    if not num:
        nm = re.search(r"(\d+)", file_stem)
        num = nm.group(1) if nm else file_stem
    return f"law_{num}_{year}"

def extract_references(preamble, body_text):
    all_text = " ".join(preamble) + " " + body_text
    law_pat = re.compile(r"Ù‚Ø§Ù†ÙˆÙ†\s+Ø±Ù‚Ù…\s*\((\d+)\)\s*Ù„Ø³Ù†Ø©\s*(\d{4})")
    refs = list(dict.fromkeys(
        f"law_{m.group(1)}_{m.group(2)}" for m in law_pat.finditer(all_text)
    ))
    amends  = refs if ("ÙŠØ³ØªØ¨Ø¯Ù„" in all_text or "Ø¨ØªØ¹Ø¯ÙŠÙ„" in all_text) else []
    repeals = refs if ("ÙŠÙÙ„ØºÙ‰" in all_text or "ÙŠÙ„ØºÙ‰" in all_text) else []
    return {"amends": amends, "repeals": repeals, "references": refs}

def enhance_law(data, file_stem):
    meta     = data.get("Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹", {})
    preamble = data.get("Ø¯ÛŒØ¨Ø§Ø¬Ø©", [])
    law_id   = build_law_id(file_stem, meta)

    num = meta.get("Ø±Ù‚Ù…", "")
    year_raw = meta.get("Ø§Ù„ØªØ§Ø±ÛŒØ®", meta.get("Ø§Ù„ØªØ§Ø±ÙŠØ®", ""))
    year_m = re.search(r"(\d{4})", year_raw)
    year = year_m.group(1) if year_m else ""
    title = f"Ù‚Ø§Ù†ÙˆÙ† Ø±Ù‚Ù… ({num}) Ù„Ø³Ù†Ø© {year}" if num and year else ""

    meta["_law_id"]   = law_id
    meta["_title"]    = title
    meta["_law_type"] = infer_law_type(title + " ".join(preamble))

    all_body = []
    for ch in data.get("ÙØµÙˆÙ„", []):
        for art in ch.get("Ù…ÙˆØ§Ø¯", []):
            all_body.extend(art.get("Ù†Øµ", []))
    for art in data.get("Ù…ÙˆØ§Ø¯", []):
        all_body.extend(art.get("Ù†Øµ", []))

    refs = extract_references(preamble, " ".join(all_body))
    meta["_amends"]     = refs["amends"]
    meta["_repeals"]    = refs["repeals"]
    meta["_references"] = refs["references"]

    def enrich_article(art, chapter_title=""):
        full_text = normalize(" ".join(art.get("Ù†Øµ", [])))
        art["full_text"]      = full_text
        art["article_number"] = extract_article_number(art.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©", ""))
        art["chapter_title"]  = chapter_title
        art["keywords"]       = extract_keywords(full_text)
        art["char_count"]     = len(full_text)
        art["embedding_text"] = (
            f"Ù‚Ø§Ù†ÙˆÙ†: {title}\n"
            + (f"Ø§Ù„ÙØµÙ„: {chapter_title}\n" if chapter_title else "")
            + f"Ø§Ù„Ù…Ø§Ø¯Ø©: {art.get('Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©', '')}\n"
            + f"Ø§Ù„Ù†Øµ: {full_text}"
        )
        art["law_id"] = law_id
        return art

    for i, ch in enumerate(data.get("ÙØµÙˆÙ„", []), 1):
        ch["chapter_index"] = i
        for art in ch.get("Ù…ÙˆØ§Ø¯", []):
            enrich_article(art, ch.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„ÙØµÙ„", ""))
    for art in data.get("Ù…ÙˆØ§Ø¯", []):
        enrich_article(art, "")

    data["Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹"] = meta
    return data

def e(s):
    """Escape string for Cypher."""
    if s is None: return ""
    return str(s).replace("\\","\\\\").replace("'","\\'").replace("\n"," ")

def law_to_cypher(data):
    meta    = data.get("Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹", {})
    law_id  = e(meta.get("_law_id"))
    title   = e(meta.get("_title", ""))
    status  = e(meta.get("Ø§Ù„Ø­Ø§Ù„Ø©",""))
    pub     = e(meta.get("Ø§Ù„Ù†Ø´Ø±",""))
    num     = e(meta.get("Ø±Ù‚Ù…",""))
    preamble= e(" ".join(data.get("Ø¯ÛŒØ¨Ø§Ø¬Ø©",[])))
    ltype   = e(meta.get("_law_type","original"))

    stmts = [
        "CREATE CONSTRAINT law_id IF NOT EXISTS FOR (l:Law) REQUIRE l.law_id IS UNIQUE;",
        "CREATE CONSTRAINT article_uid IF NOT EXISTS FOR (a:Article) REQUIRE a.uid IS UNIQUE;",
        "CREATE CONSTRAINT chapter_uid IF NOT EXISTS FOR (c:Chapter) REQUIRE c.uid IS UNIQUE;",
        f"""MERGE (l:Law {{law_id: '{law_id}'}})
SET l.title='{title}', l.law_type='{ltype}', l.status='{status}',
    l.pub_date='{pub}', l.number='{num}', l.preamble='{preamble}',
    l.updated_at=datetime();""",
    ]

    def add_article(art, ch_uid=None):
        uid     = f"{law_id}_art_{e(art.get('article_number',''))}"
        title_a = e(art.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©",""))
        ftxt    = e(art.get("full_text",""))
        etxt    = e(art.get("embedding_text",""))
        kws     = ",".join(f"'{e(k)}'" for k in art.get("keywords",[]))
        ch_t    = e(art.get("chapter_title",""))
        stmts.append(f"""MERGE (a:Article {{uid:'{uid}'}})
SET a.title='{title_a}', a.article_number='{e(art.get("article_number",""))}',
    a.full_text='{ftxt}', a.embedding_text='{etxt}',
    a.chapter_title='{ch_t}', a.keywords=[{kws}],
    a.char_count={art.get("char_count",0)}, a.law_id='{law_id}';""")
        if ch_uid:
            stmts.append(f"MATCH (ch:Chapter {{uid:'{ch_uid}'}}),(a:Article {{uid:'{uid}'}}) MERGE (ch)-[:HAS_ARTICLE]->(a);")
        else:
            stmts.append(f"MATCH (l:Law {{law_id:'{law_id}'}}),(a:Article {{uid:'{uid}'}}) MERGE (l)-[:HAS_ARTICLE]->(a);")

    for ch in data.get("ÙØµÙˆÙ„",[]):
        ch_uid = f"{law_id}_ch_{ch.get('chapter_index',0)}"
        ch_t   = e(ch.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„ÙØµÙ„",""))
        ch_i   = ch.get("chapter_index",0)
        stmts.append(f"""MERGE (ch:Chapter {{uid:'{ch_uid}'}})
SET ch.title='{ch_t}', ch.chapter_index={ch_i}, ch.law_id='{law_id}';""")
        stmts.append(f"MATCH (l:Law {{law_id:'{law_id}'}}),(ch:Chapter {{uid:'{ch_uid}'}}) MERGE (l)-[:HAS_CHAPTER]->(ch);")
        for art in ch.get("Ù…ÙˆØ§Ø¯",[]):
            add_article(art, ch_uid)

    for art in data.get("Ù…ÙˆØ§Ø¯",[]):
        add_article(art, None)

    for ref in meta.get("_amends",[]):
        stmts.append(f"MERGE (r:Law {{law_id:'{e(ref)}'}}); MATCH (l:Law {{law_id:'{law_id}'}}),(r:Law {{law_id:'{e(ref)}'}}) MERGE (l)-[:AMENDS]->(r);")
    for ref in meta.get("_repeals",[]):
        stmts.append(f"MERGE (r:Law {{law_id:'{e(ref)}'}}); MATCH (l:Law {{law_id:'{law_id}'}}),(r:Law {{law_id:'{e(ref)}'}}) MERGE (l)-[:REPEALS]->(r);")

    return stmts

def insert_to_neo4j(all_statements):
    try:
        from neo4j import GraphDatabase
    except ImportError:
        print("âŒ Run: pip install neo4j")
        return False

    print(f"Connecting to {NEO4J_URI}...")
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    total, errors = 0, 0
    try:
        with driver.session() as session:
            for stmt in all_statements:
                stmt = stmt.strip()
                if not stmt: continue
                # Split multiple statements separated by ;
                parts = [p.strip() for p in stmt.split(";") if p.strip()]
                for part in parts:
                    try:
                        session.run(part)
                        total += 1
                    except Exception as ex:
                        errors += 1
                        print(f"  âš ï¸  {ex}")
    finally:
        driver.close()
    print(f"  âœ… {total} statements executed, {errors} errors")
    return errors == 0

def export_jsonl(all_laws):
    out = OUTPUT_DIR / "articles_for_embedding.jsonl"
    count = 0
    with open(out, "w", encoding="utf-8") as f:
        for data in all_laws:
            meta = data.get("Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹",{})
            for ch in data.get("ÙØµÙˆÙ„",[]):
                for art in ch.get("Ù…ÙˆØ§Ø¯",[]):
                    f.write(json.dumps({
                        "law_id":         meta.get("_law_id",""),
                        "law_title":      meta.get("_title",""),
                        "chapter":        art.get("chapter_title",""),
                        "article_number": art.get("article_number",""),
                        "article_title":  art.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©",""),
                        "full_text":      art.get("full_text",""),
                        "embedding_text": art.get("embedding_text",""),
                        "keywords":       art.get("keywords",[]),
                    }, ensure_ascii=False) + "\n")
                    count += 1
            for art in data.get("Ù…ÙˆØ§Ø¯",[]):
                f.write(json.dumps({
                    "law_id":         meta.get("_law_id",""),
                    "law_title":      meta.get("_title",""),
                    "chapter":        "",
                    "article_number": art.get("article_number",""),
                    "article_title":  art.get("Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©",""),
                    "full_text":      art.get("full_text",""),
                    "embedding_text": art.get("embedding_text",""),
                    "keywords":       art.get("keywords",[]),
                }, ensure_ascii=False) + "\n")
                count += 1
    print(f"  âœ… {count} articles â†’ {out}")

def main():
    json_files = sorted(JSON_DIR.glob("output*.json"))
    if not json_files:
        print(f"âŒ No JSON files in {JSON_DIR}/  â€” run step1 first!")
        sys.exit(1)

    all_laws, all_stmts = [], []

    for jf in json_files:
        print(f"\nğŸ“„ {jf.name}")
        with open(jf, encoding="utf-8") as fp:
            data = json.load(fp)

        data = enhance_law(data, jf.stem)
        meta = data["Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹"]
        print(f"  law_id  : {meta['_law_id']}")
        print(f"  type    : {meta['_law_type']}")
        print(f"  amends  : {meta['_amends']}")

        # Save enhanced JSON
        enh = OUTPUT_DIR / f"enhanced_{jf.name}"
        with open(enh, "w", encoding="utf-8") as fp:
            json.dump(data, fp, ensure_ascii=False, indent=2)

        stmts = law_to_cypher(data)

        # Save individual Cypher file
        cypher_path = OUTPUT_DIR / f"cypher_{jf.stem}.cypher"
        with open(cypher_path, "w", encoding="utf-8") as fp:
            fp.write("\n\n".join(stmts))
        print(f"  âœ… Cypher saved â†’ {cypher_path}")

        all_laws.append(data)
        all_stmts.extend(stmts)

    # Save combined Cypher
    combined = OUTPUT_DIR / "all_laws.cypher"
    with open(combined, "w", encoding="utf-8") as f:
        f.write("\n\n".join(all_stmts))
    print(f"\nâœ… Combined Cypher â†’ {combined}  ({len(all_stmts)} statements)")

    # Export JSONL for embeddings
    export_jsonl(all_laws)

    # Insert into Neo4j
    print(f"\nğŸ”Œ Inserting into Neo4j...")
    insert_to_neo4j(all_stmts)

    print("\nâœ… Done! Your graph is ready.")

if __name__ == "__main__":
    main()