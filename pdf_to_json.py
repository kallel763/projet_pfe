import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import unicodedata
import re
import json
import os

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PDF_INPUT = "law19.pdf"
OUTPUT_JSON = "output19.json"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: EXTRACT TEXT FROM PDF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def extract_text(pdf_path):
    print("ğŸ“„ Step 1: Extracting text from PDF...")
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    print("âœ… Text extracted successfully")
    return text

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: FIX ARABIC BIDI + RESHAPING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fix_arabic(text):
    print("ğŸ”¤ Step 2: Fixing Arabic text (reshape + bidi)...")
    reshaped_text = arabic_reshaper.reshape(text)
    fixed_text = get_display(reshaped_text)
    print("âœ… Arabic text corrected")
    return fixed_text

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: NORMALIZE UNICODE + DIGITS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def normalize_text(text):
    print("ğŸ”§ Step 3: Normalizing Unicode...")
    text = unicodedata.normalize("NFKC", text)

    arabic_digits = {
        "Ù ": "0", "Ù¡": "1", "Ù¢": "2", "Ù£": "3", "Ù¤": "4",
        "Ù¥": "5", "Ù¦": "6", "Ù§": "7", "Ù¨": "8", "Ù©": "9"
    }
    for k, v in arabic_digits.items():
        text = text.replace(k, v)

    print("âœ… Unicode normalization done")
    return text

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 4: CLEAN TEXT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def clean_text(text):
    print("ğŸ§¹ Step 4: Cleaning text...")
    text = unicodedata.normalize("NFKC", text)

    # Remove almeezan URL + page number
    text = re.sub(r'https?://www\.almeezan\.qa/\S+\s*\d+/\d+', '', text)

    # Remove repeated law title + date on each page
    text = re.sub(
        r'.*(?:ï»—ïºï»§ï»®ï»§|Ù‚Ø§Ù†ÙˆÙ†)\s*(?:ïº­ï»—ï»¡|Ø±Ù‚Ù…)\s*\(13\)\s*(?:ï»Ÿïº³ï»§ïº”|Ù„Ø³Ù†Ø©)\s*2024\s*(?:ïº‘ïº·ïº„ï»¥|Ø¨Ø´Ø£Ù†).*?(?:10[:/]02[:/]2026|10:41).*',
        '', text
    )
    text = re.sub(
        r'.*(?:ï»—ïºï»§ï»®ï»§|Ù‚Ø§Ù†ÙˆÙ†)\s*(?:ïº­ï»—ï»¡|Ø±Ù‚Ù…)\s*\(13\)\s*(?:ï»Ÿïº³ï»§ïº”|Ù„Ø³Ù†Ø©)\s*2024\s*(?:ïº‘ïº·ïº„ï»¥|Ø¨Ø´Ø£Ù†).*(?:ïºï»Ÿï»Œïºï»£ïº”|Ø§Ù„Ø¹Ø§Ù…Ø©).*\n?',
        '', text
    )

    # Remove standalone date/time
    text = re.sub(r'\d{2}/\d{2}/\d{4}\s*\d{1,2}:\d{2}', '', text)

    # Remove footer
    text = re.sub(r'.*Ø§Ù„Ø±Ø¬Ø§Ø¡\s+Ø¹Ø¯Ù…\s+Ø§Ø¹ØªØ¨Ø§Ø±.*', '', text)
    text = re.sub(r'.*ïºï»Ÿïº­ïºŸïºïº€\s+.*(?:ïº­ïº³ï»£ï¯¾ïº”|Ø±Ø³Ù…ÙŠØ©).*', '', text)
    text = re.sub(r'Â©.*$', '', text, flags=re.MULTILINE)

    # Clean up whitespace
    text = re.sub(r'\r\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()

    print(f"âœ… Cleaned text: {len(text)} characters")
    return text

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 5: CREATE JSON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHAPTER_ORDINALS = (
    "Ø§Ù„Ø£ÙˆÙ„", "Ø§Ù„Ø«Ø§Ù†ÙŠ", "Ø§Ù„Ø«Ø§Ù„Ø«", "Ø§Ù„Ø±Ø§Ø¨Ø¹", "Ø§Ù„Ø®Ø§Ù…Ø³",
    "Ø§Ù„Ø³Ø§Ø¯Ø³", "Ø§Ù„Ø³Ø§Ø¨Ø¹", "Ø§Ù„Ø«Ø§Ù…Ù†", "Ø§Ù„ØªØ§Ø³Ø¹", "Ø§Ù„Ø¹Ø§Ø´Ø±",
)

def strip_tatweel(text):
    return text.replace("Ù€", "")

def is_section_heading(text):
    clean = strip_tatweel(text).strip()
    for keyword in ("Ø§Ù„ÙØµÙ„", "Ø§Ù„Ø¨Ø§Ø¨"):
        if clean.startswith(keyword):
            if any(ordinal in clean for ordinal in CHAPTER_ORDINALS):
                return True
    return False

def is_toc_line(raw_line):
    stripped = raw_line.rstrip()
    if re.match(r"^[\uf0da]\s*", stripped):
        return True
    if stripped.startswith(" "):
        inner = stripped.strip()
        if is_section_heading(inner):
            return True
        if re.match(r"^Ù…ÙˆØ§Ø¯\s", inner):
            return True
    return False

ARTICLE_PATTERN = re.compile(
    r"^Ø§Ù„Ù…Ø§Ø¯Ø©\s*"
    r"(-\s*[\dÙ -Ù©]+|[\dÙ -Ù©]+|Ø§Ù„\S+(\s+\S+){0,3})"
)

def is_article_line(text):
    return bool(ARTICLE_PATTERN.match(text.strip()))

PREAMBLE_START        = re.compile(r"^Ù†Ø­Ù†\s")
PREAMBLE_END_EXPLICIT = re.compile(r"Ù‚Ø±Ø±Ù†Ø§ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†")

def _flush_article(law, current_chapter, current_article):
    if current_article is None:
        return
    if current_chapter is not None:
        current_chapter["Ù…ÙˆØ§Ø¯"].append(current_article)
    else:
        law["Ù…ÙˆØ§Ø¯"].append(current_article)

def parse_metadata_line(text, law):
    text = re.sub(r"Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ´Ø±[ÛŒÙŠ]Ø¹", "", text).strip()
    pairs = re.findall(
        r"([\u0600-\u06FF\s]+?):\s*([^:]+?)(?=\s+[\u0600-\u06FF]+:|$)", text
    )
    if pairs:
        for key, value in pairs:
            law["Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹"][key.strip()] = value.strip()
    elif ":" in text:
        key, value = text.split(":", 1)
        law["Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹"][key.strip()] = value.strip()

def create_json(structured_text, output_path):
    print("ğŸ“¦ Step 5: Creating JSON...")

    lines = structured_text.split("\n")

    law = {
        "Ø¨Ø·Ø§Ù‚Ø©_Ø§Ù„ØªØ´Ø±ÛŒØ¹": {},
        "ÙÚ¾Ø±Ø³":  [],
        "Ø¯ÛŒØ¨Ø§Ø¬Ø©": [],
        "ÙØµÙˆÙ„":  [],
        "Ù…ÙˆØ§Ø¯":  [],
    }

    current_chapter = None
    current_article = None
    section = "metadata"

    for raw_line in lines:
        text    = raw_line.rstrip("\n")
        stripped = text.strip()

        if not stripped or re.match(r"^[â€¢\s]+$", stripped):
            continue

        clean = re.sub(r"^[â€¢\s]+", "", stripped).strip()
        clean = re.sub(r"[â€¢]+$",   "", clean).strip()
        if not clean:
            continue

        is_sec = is_section_heading(clean)
        is_art = is_article_line(clean)

        # â”€â”€ TOC
        if section in ("metadata", "toc"):
            if is_toc_line(text):
                toc_text = re.sub(r"^[\uf0da\s]+", "", text).strip()
                law["ÙÚ¾Ø±Ø³"].append(toc_text)
                section = "toc"
                continue

        if section in ("metadata", "toc"):
            if PREAMBLE_START.match(clean):
                section = "preamble"
                law["Ø¯ÛŒØ¨Ø§Ø¬Ø©"].append(clean)
                continue
            elif is_sec or is_art:
                section = "body"
            elif section == "metadata":
                if "Ù‚Ø§Ù†ÙˆÙ† Ø±Ù‚Ù…" in clean:
                    continue
                parse_metadata_line(clean, law)
                continue
            else:
                continue

        # â”€â”€ Preamble
        if section == "preamble":
            if "Ù‚Ø§Ù†ÙˆÙ† Ø±Ù‚Ù…" in clean \
                    and "Ù†Ø­Ù†" not in clean and "ÙˆØ¹Ù„Ù‰" not in clean:
                continue

            if PREAMBLE_END_EXPLICIT.search(clean):
                law["Ø¯ÛŒØ¨Ø§Ø¬Ø©"].append(clean)
                section = "body"
                continue

            if is_sec:
                section = "body"
            else:
                law["Ø¯ÛŒØ¨Ø§Ø¬Ø©"].append(clean)
                continue

        # â”€â”€ Body
        if section == "body":
            if "Ù‚Ø§Ù†ÙˆÙ† Ø±Ù‚Ù…" in clean and not is_art and not is_sec:
                continue

            if is_sec:
                _flush_article(law, current_chapter, current_article)
                current_article = None
                if current_chapter:
                    law["ÙØµÙˆÙ„"].append(current_chapter)
                current_chapter = {"Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„ÙØµÙ„": clean, "Ù…ÙˆØ§Ø¯": []}

            elif is_art:
                _flush_article(law, current_chapter, current_article)
                current_article = {"Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ù…Ø§Ø¯Ø©": clean, "Ù†Øµ": []}

            elif current_article is not None:
                current_article["Ù†Øµ"].append(clean)

            elif current_chapter is not None and current_article is None:
                if len(clean) <= 60:
                    current_chapter["Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„ÙØµÙ„"] += " - " + clean

    # Flush last article and chapter
    _flush_article(law, current_chapter, current_article)
    if current_chapter:
        law["ÙØµÙˆÙ„"].append(current_chapter)

    if not law["Ù…ÙˆØ§Ø¯"]:
        del law["Ù…ÙˆØ§Ø¯"]

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(law, f, ensure_ascii=False, indent=4)

    total_chapters = len(law.get("ÙØµÙˆÙ„", []))
    total_arts_in  = sum(len(ch["Ù…ÙˆØ§Ø¯"]) for ch in law.get("ÙØµÙˆÙ„", []))
    total_arts_top = len(law.get("Ù…ÙˆØ§Ø¯", []))
    total_articles = total_arts_in + total_arts_top

    print(f"âœ… JSON created: {output_path}")
    if total_chapters:
        print(f"   Chapters: {total_chapters}")
    else:
        print("   No chapters â€” articles at top level")
    print(f"   Articles: {total_articles}")

    return law

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    print("=" * 50)
    print("  LAW PDF â†’ JSON PIPELINE")
    print("=" * 50)
    PDF_INPUT = "law19.pdf"
    if not os.path.exists(PDF_INPUT):
        print(f"âŒ PDF not found: {PDF_INPUT}")
        return

    # Step 1: Extract
    raw_text = extract_text(PDF_INPUT)

    # Step 2: Fix Arabic
    fixed = fix_arabic(raw_text)

    # Step 3: Normalize
    normalized = normalize_text(fixed)

    # Step 4: Clean
    cleaned = clean_text(normalized)

    # Step 5: Create JSON
    law = create_json(cleaned, OUTPUT_JSON)

    print("\n" + "=" * 50)
    print("  âœ… PIPELINE COMPLETE")
    print("=" * 50)

if __name__ == "__main__":
    main()